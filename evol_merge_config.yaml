genome:
    models:
      - google/gemma-1.1-7b-it
      - google/gemma-1.1-2b-it
      - microsoft/Phi-3-mini-128k-instruct
      - google/codegemma-7b-it
      - cognitivecomputations/dolphin-2.6-mistral-7b
      - meta-math/MetaMath-Mistral-7B
      - Rakuten/RakutenAI-7B-chat
      - mistralai/Mistral-7B-Instruct-v0.2
    merge_method: task_arithmetic
    base_model: mistralai/Mistral-7B-Instruct-v0.2
    # tokenizer_source: base
    layer_granularity: 8 # sane default
    allow_negative_weights: true # useful with task_arithmetic
tasks:
  - name: alpaca_prompt_format
    weight: 0.4
  - name: spartqa_train
    weight: 0.6
